{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cb6213",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934c5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd3a10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- !!! IMPORTANT: CONFIGURE THESE VARIABLES !!! ---\n",
    "\n",
    "# 1. The path to your new, unseen CSV file\n",
    "INPUT_CSV_FILE = 'G:/My Drive/University Files/5th Semester/Data Science/Project/IndoHoaxDetector/data/tweets_from_kompascom_20251106_124453.csv'\n",
    "\n",
    "# 2. The name of the column in your CSV that contains the main text\n",
    "TEXT_COLUMN_NAME = 'text' \n",
    "\n",
    "# 3. (Optional) The name of the title column, if you want to include it.\n",
    "#    Set to None if you don't have a title column.\n",
    "TITLE_COLUMN_NAME = 'title' \n",
    "\n",
    "# 4. The name of the file to save your results to\n",
    "OUTPUT_CSV_FILE = 'prediction_results_tweets_from_kompascom_20251106_124453.csv'\n",
    "\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87dabcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up preprocessing tools...\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup Preprocessing Tools ---\n",
    "print(\"Setting up preprocessing tools...\")\n",
    "try:\n",
    "    # Download NLTK stopwords if not already present\n",
    "    stopwords.words('indonesian')\n",
    "except LookupError:\n",
    "    print(\"NLTK stopwords not found. Downloading...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Initialize Stemmer (can take a moment)\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Get Indonesian stop words\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    The exact same cleaning function used during training.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            cleaned_tokens.append(stemmed_word)\n",
    "    return ' '.join(cleaned_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a909bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and vectorizer...\n",
      "Files loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load Model and Vectorizer ---\n",
    "print(\"Loading saved model and vectorizer...\")\n",
    "try:\n",
    "    model = joblib.load('logreg_model.pkl')\n",
    "    vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'logreg_model.pkl' or 'tfidf_vectorizer.pkl'.\")\n",
    "    print(\"Make sure these files are in the same folder as this script.\")\n",
    "    sys.exit()\n",
    "print(\"Files loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d18afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading new data from 'G:/My Drive/University Files/5th Semester/Data Science/Project/IndoHoaxDetector/data/tweets_from_kompascom_20251106_124453.csv'...\n",
      "Preparing text for preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load and Process New CSV File ---\n",
    "print(f\"Loading new data from '{INPUT_CSV_FILE}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{INPUT_CSV_FILE}' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Check if the required text column exists\n",
    "if TEXT_COLUMN_NAME not in df.columns:\n",
    "    print(f\"Error: Column '{TEXT_COLUMN_NAME}' not found in your CSV.\")\n",
    "    print(f\"Available columns are: {list(df.columns)}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a combined text column for processing\n",
    "print(\"Preparing text for preprocessing...\")\n",
    "# Ensure text column is string and fill missing values\n",
    "df['text_to_process'] = df[TEXT_COLUMN_NAME].astype(str).fillna('')\n",
    "\n",
    "# Prepend title if it exists and is specified\n",
    "if TITLE_COLUMN_NAME and TITLE_COLUMN_NAME in df.columns:\n",
    "    df['title_str'] = df[TITLE_COLUMN_NAME].astype(str).fillna('')\n",
    "    df['text_to_process'] = df['title_str'] + ' ' + df['text_to_process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e57fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning new text... (This may take a while for large files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Text: 100%|██████████| 5000/5000 [18:21<00:00,  4.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text (using loaded TF-IDF)...\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Preprocess and Predict ---\n",
    "print(\"Cleaning new text... (This may take a while for large files)\")\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "df['text_clean'] = df['text_to_process'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"Vectorizing text (using loaded TF-IDF)...\")\n",
    "# IMPORTANT: Use .transform() only. DO NOT use .fit_transform()\n",
    "X_new = vectorizer.transform(df['text_clean'])\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "# Predict the labels (0 or 1)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Get the confidence probabilities\n",
    "probabilities = model.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4bace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting results...\n",
      "\n",
      "--- Batch Prediction Complete ---\n",
      "Results saved to 'prediction_results_tweets_from_kompascom_20251106_124453.csv'\n",
      "\n",
      "Preview of results:\n",
      "                    id                                               text  \\\n",
      "0  1986283688972460183  Asosiasi Sepak Bola Malaysia (FAM) berpotensi ...   \n",
      "1  1986281660158255211  Di balik ceritanya yang menusuk dan sinematogr...   \n",
      "2  1986281407132672265  Presiden Meksiko Claudia Sheinbaum mengalami p...   \n",
      "3  1986281320046338235  Berdasarkan Laporan Harta Kekayaan Penyelengga...   \n",
      "4  1986279330906071392  Otoritas Portugal menangkap empat orang setela...   \n",
      "\n",
      "                       created_at        user   username  retweet_count  \\\n",
      "0  Thu Nov 06 04:05:09 +0000 2025  Kompas.com  kompascom              0   \n",
      "1  Thu Nov 06 03:57:05 +0000 2025  Kompas.com  kompascom              0   \n",
      "2  Thu Nov 06 03:56:05 +0000 2025  Kompas.com  kompascom              0   \n",
      "3  Thu Nov 06 03:55:44 +0000 2025  Kompas.com  kompascom              0   \n",
      "4  Thu Nov 06 03:47:50 +0000 2025  Kompas.com  kompascom              0   \n",
      "\n",
      "   favorite_count  view_count  reply_count  quote_count lang prediction  \\\n",
      "0               0           7            0            0   in       HOAX   \n",
      "1               1         233            0            0   in      FAKTA   \n",
      "2               0         291            0            1   in      FAKTA   \n",
      "3               1         255            0            0   in      FAKTA   \n",
      "4               0         427            0            0   in       HOAX   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.514507  \n",
      "1          0.580468  \n",
      "2          0.899029  \n",
      "3          0.920609  \n",
      "4          0.726242  \n"
     ]
    }
   ],
   "source": [
    "# --- 5. Format and Save Results ---\n",
    "print(\"Formatting results...\")\n",
    "# Add predictions to the DataFrame\n",
    "df['predicted_label'] = predictions\n",
    "df['prediction'] = df['predicted_label'].map({0: 'FAKTA', 1: 'HOAX'})\n",
    "\n",
    "# Add the confidence score for the predicted class\n",
    "df['confidence_score'] = probabilities.max(axis=1)\n",
    "\n",
    "# Select columns to save\n",
    "columns_to_save = [col for col in df.columns if col not in \n",
    "                   ['text_to_process', 'title_str', 'text_clean', 'predicted_label']]\n",
    "\n",
    "final_df = df[columns_to_save]\n",
    "\n",
    "# Save the final results\n",
    "final_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Batch Prediction Complete ---\")\n",
    "print(f\"Results saved to '{OUTPUT_CSV_FILE}'\")\n",
    "print(\"\\nPreview of results:\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
