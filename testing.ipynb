{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e57fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gareth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and vectorizer...\n",
      "Files loaded successfully.\n",
      "Loading new data from 'data/tweets_from_cekfaktacom_20251105_144313.csv'...\n",
      "Preparing text for preprocessing...\n",
      "Cleaning new text... (This may take a while for large files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Text: 100%|██████████| 1990/1990 [03:03<00:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text (using loaded TF-IDF)...\n",
      "Making predictions...\n",
      "Formatting results...\n",
      "\n",
      "--- Batch Prediction Complete ---\n",
      "Results saved to 'prediction_results.csv'\n",
      "\n",
      "Preview of results:\n",
      "                    id                                               text  \\\n",
      "0  1966029663010197798  Baca artikel lengkap penelusuran kami di tauta...   \n",
      "1  1966029471858962914  Benarkah ada artikel berita milik detikcom yan...   \n",
      "2  1966029268821110886  Sebuah narasi beredar menyebut bahwa “Anies Ba...   \n",
      "3  1964980465879891996  (2/2) \\nBaca artikel lengkap penelusuran kami ...   \n",
      "4  1964980252867985492  Sebuah unggahan video di TikTok dan Facebook m...   \n",
      "\n",
      "                       created_at         user     username  retweet_count  \\\n",
      "0  Thu Sep 11 06:42:53 +0000 2025  cekfaktacom  cekfaktacom              0   \n",
      "1  Thu Sep 11 06:42:07 +0000 2025  cekfaktacom  cekfaktacom              0   \n",
      "2  Thu Sep 11 06:41:19 +0000 2025  cekfaktacom  cekfaktacom              0   \n",
      "3  Mon Sep 08 09:13:45 +0000 2025  cekfaktacom  cekfaktacom              1   \n",
      "4  Mon Sep 08 09:12:54 +0000 2025  cekfaktacom  cekfaktacom              1   \n",
      "\n",
      "   favorite_count  view_count  reply_count  quote_count lang prediction  \\\n",
      "0               0       142.0            0            0   in       HOAX   \n",
      "1               0       182.0            1            0   in       HOAX   \n",
      "2               0       200.0            1            0   in       HOAX   \n",
      "3               1        33.0            0            0   in       HOAX   \n",
      "4               1       188.0            1            0   in       HOAX   \n",
      "\n",
      "   confidence_score  \n",
      "0          0.997221  \n",
      "1          0.995789  \n",
      "2          0.998086  \n",
      "3          0.997221  \n",
      "4          0.912566  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load Model and Vectorizer ---\n",
    "print(\"Loading saved model and vectorizer...\")\n",
    "try:\n",
    "    model = joblib.load('logreg_model.pkl')\n",
    "    vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find 'logreg_model.pkl' or 'tfidf_vectorizer.pkl'.\")\n",
    "    print(\"Make sure these files are in the same folder as this script.\")\n",
    "    sys.exit()\n",
    "print(\"Files loaded successfully.\")\n",
    "\n",
    "# --- 3. Load and Process New CSV File ---\n",
    "print(f\"Loading new data from '{INPUT_CSV_FILE}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{INPUT_CSV_FILE}' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Check if the required text column exists\n",
    "if TEXT_COLUMN_NAME not in df.columns:\n",
    "    print(f\"Error: Column '{TEXT_COLUMN_NAME}' not found in your CSV.\")\n",
    "    print(f\"Available columns are: {list(df.columns)}\")\n",
    "    sys.exit()\n",
    "\n",
    "# Create a combined text column for processing\n",
    "print(\"Preparing text for preprocessing...\")\n",
    "# Ensure text column is string and fill missing values\n",
    "df['text_to_process'] = df[TEXT_COLUMN_NAME].astype(str).fillna('')\n",
    "\n",
    "# Prepend title if it exists and is specified\n",
    "if TITLE_COLUMN_NAME and TITLE_COLUMN_NAME in df.columns:\n",
    "    df['title_str'] = df[TITLE_COLUMN_NAME].astype(str).fillna('')\n",
    "    df['text_to_process'] = df['title_str'] + ' ' + df['text_to_process']\n",
    "\n",
    "# --- 4. Preprocess and Predict ---\n",
    "print(\"Cleaning new text... (This may take a while for large files)\")\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "df['text_clean'] = df['text_to_process'].progress_apply(preprocess_text)\n",
    "\n",
    "print(\"Vectorizing text (using loaded TF-IDF)...\")\n",
    "# IMPORTANT: Use .transform() only. DO NOT use .fit_transform()\n",
    "X_new = vectorizer.transform(df['text_clean'])\n",
    "\n",
    "print(\"Making predictions...\")\n",
    "# Predict the labels (0 or 1)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Get the confidence probabilities\n",
    "probabilities = model.predict_proba(X_new)\n",
    "\n",
    "# --- 5. Format and Save Results ---\n",
    "print(\"Formatting results...\")\n",
    "# Add predictions to the DataFrame\n",
    "df['predicted_label'] = predictions\n",
    "df['prediction'] = df['predicted_label'].map({0: 'FAKTA', 1: 'HOAX'})\n",
    "\n",
    "# Add the confidence score for the predicted class\n",
    "df['confidence_score'] = probabilities.max(axis=1)\n",
    "\n",
    "# Select columns to save\n",
    "columns_to_save = [col for col in df.columns if col not in \n",
    "                   ['text_to_process', 'title_str', 'text_clean', 'predicted_label']]\n",
    "\n",
    "final_df = df[columns_to_save]\n",
    "\n",
    "# Save the final results\n",
    "final_df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "\n",
    "print(\"\\n--- Batch Prediction Complete ---\")\n",
    "print(f\"Results saved to '{OUTPUT_CSV_FILE}'\")\n",
    "print(\"\\nPreview of results:\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
