{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndoHoaxDetector Model Evaluation\n",
    "\n",
    "This notebook evaluates the IndoHoaxDetector model (TF-IDF + Logistic Regression) on labeled data.\n",
    "\n",
    "It loads the trained model and vectorizer, evaluates on preprocessed labeled data, and provides:\n",
    "- Accuracy, F1 scores, confusion matrix\n",
    "- Error analysis with high-confidence false positives/negatives\n",
    "\n",
    "Defaults are set for your project files. Run all cells to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set paths and column names. Defaults are for your IndoHoaxDetector project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default paths (adjust if needed)\n",
    "DEFAULT_MODEL_PATH = \"logreg_model.pkl\"\n",
    "DEFAULT_VECTORIZER_PATH = \"tfidf_vectorizer.pkl\"\n",
    "\n",
    "# Expected labeled CSV columns\n",
    "DEFAULT_TEXT_COL = \"text_clean\"\n",
    "DEFAULT_LABEL_COL = \"label_encoded\"\n",
    "\n",
    "INT_TO_STRING_LABEL = {0: \"FAKTA\", 1: \"HOAX\"}\n",
    "\n",
    "# Data path (update to your labeled CSV)\n",
    "DATA_PATH = \"g:/My Drive/University Files/5th Semester/Data Science/Project/preprocessed_data_FINAL_FINAL.csv\"\n",
    "TEXT_COL = DEFAULT_TEXT_COL\n",
    "LABEL_COL = DEFAULT_LABEL_COL\n",
    "MODEL_PATH = DEFAULT_MODEL_PATH\n",
    "VECTORIZER_PATH = DEFAULT_VECTORIZER_PATH\n",
    "MAX_SHOW = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_vectorizer(\n",
    "    model_path: str,\n",
    "    vectorizer_path: str,\n",
    "):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"[ERROR] Model file not found: {model_path}\", file=sys.stderr)\n",
    "        return None, None\n",
    "    if not os.path.exists(vectorizer_path):\n",
    "        print(f\"[ERROR] Vectorizer file not found: {vectorizer_path}\", file=sys.stderr)\n",
    "        return None, None\n",
    "\n",
    "    print(f\"[INFO] Loading model from: {model_path}\")\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"[INFO] Loading vectorizer from: {vectorizer_path}\")\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "    return model, vectorizer\n",
    "\n",
    "\n",
    "def load_labeled_data(\n",
    "    csv_path: str,\n",
    "    text_col: str,\n",
    "    label_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load labeled evaluation data.\n",
    "\n",
    "    Assumptions for your project:\n",
    "    - text_col = 'text_clean' (already preprocessed exactly as during training)\n",
    "    - label_col = 'label_encoded' (0 = FAKTA, 1 = HOAX)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"[ERROR] Labeled CSV not found: {csv_path}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"[INFO] Loading labeled data from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if text_col not in df.columns:\n",
    "        print(f\"[ERROR] Text column '{text_col}' not found. Available: {list(df.columns)}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if label_col not in df.columns:\n",
    "        print(f\"[ERROR] Label column '{label_col}' not found. Available: {list(df.columns)}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df[[text_col, label_col]].dropna()\n",
    "    if df.empty:\n",
    "        print(\"[ERROR] No valid rows after dropping NA in text/label.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # For your file, label_encoded is already 0/1, so we keep as is\n",
    "    df = df.rename(columns={text_col: \"text\", label_col: \"true_label\"})\n",
    "\n",
    "    # Keep only rows with labels 0 or 1\n",
    "    before = len(df)\n",
    "    df = df[df[\"true_label\"].isin([0, 1])]\n",
    "    after = len(df)\n",
    "    if after == 0:\n",
    "        print(\"[ERROR] No rows with valid labels (0/1) after filtering.\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "    if after < before:\n",
    "        print(f\"[INFO] Filtered out {before - after} rows with invalid label values.\")\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_labeled(\n",
    "    df: pd.DataFrame,\n",
    "    model,\n",
    "    vectorizer,\n",
    "    max_examples_to_show: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate model on already-preprocessed texts.\n",
    "\n",
    "    IMPORTANT:\n",
    "    - We DO NOT re-clean or restem here because `text_clean` in your dataset\n",
    "      is assumed to already match what the vectorizer was trained on.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Using pre-cleaned text from dataset (no extra preprocessing).\")\n",
    "\n",
    "    # Vectorize using loaded TF-IDF (MUST use transform, not fit_transform)\n",
    "    print(\"[INFO] Vectorizing texts with existing TF-IDF...\")\n",
    "    X = vectorizer.transform(df[\"text\"])\n",
    "\n",
    "    # Predictions\n",
    "    print(\"[INFO] Running predictions...\")\n",
    "    probs = model.predict_proba(X)\n",
    "    preds = model.predict(X)\n",
    "    confidences = probs.max(axis=1)\n",
    "\n",
    "    df[\"pred_label\"] = preds\n",
    "    df[\"pred_str\"] = df[\"pred_label\"].map(INT_TO_STRING_LABEL)\n",
    "    df[\"true_str\"] = df[\"true_label\"].map(INT_TO_STRING_LABEL)\n",
    "    df[\"confidence\"] = confidences\n",
    "\n",
    "    # --- Metrics ---\n",
    "    print(\"\\n===== CORE METRICS ====\")\n",
    "    acc = accuracy_score(df[\"true_label\"], df[\"pred_label\"])\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification report (macro/micro F1, per-class metrics):\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            df[\"true_label\"],\n",
    "            df[\"pred_label\"],\n",
    "            target_names=[\"FAKTA(0)\", \"HOAX(1)\"],\n",
    "            digits=4,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Confusion matrix [[TN, FP], [FN, TP]]:\")\n",
    "    print(confusion_matrix(df[\"true_label\"], df[\"pred_label\"]))\n",
    "\n",
    "    # --- Error buckets ---\n",
    "    fp = df[(df[\"true_label\"] == 0) & (df[\"pred_label\"] == 1)]\n",
    "    fn = df[(df[\"true_label\"] == 1) & (df[\"pred_label\"] == 0)]\n",
    "\n",
    "    print(f\"\\nTotal examples: {len(df)}\")\n",
    "    print(f\"False Positives (FAKTA→HOAX): {len(fp)}\")\n",
    "    print(f\"False Negatives (HOAX→FAKTA): {len(fn)}\")\n",
    "\n",
    "    # Show high-confidence mistakes for qualitative analysis\n",
    "    def show_examples(sub_df, title: str):\n",
    "        if sub_df.empty:\n",
    "            print(f\"\\nNo {title} examples.\")\n",
    "            return\n",
    "        print(f\"\\n===== {title} (up to {max_examples_to_show}) ====\")\n",
    "        sub_df_sorted = sub_df.sort_values(\"confidence\", ascending=False).head(max_examples_to_show)\n",
    "        for _, row in sub_df_sorted.iterrows():\n",
    "            snippet = str(row[\"text\"]).replace(\"\\n\", \" \")\n",
    "            if len(snippet) > 200:\n",
    "                snippet = snippet[:200] + \"...\"\n",
    "            print(\n",
    "                f\"- true={row['true_str']}, pred={row['pred_str']}, \"\n",
    "                f\"conf={row['confidence']:.3f} :: {snippet}\"\n",
    "            )\n",
    "\n",
    "    show_examples(fp, \"High-confidence False Positives\")\n",
    "    show_examples(fn, \"High-confidence False Negatives\")\n",
    "\n",
    "    print(\"\\n[INFO] Evaluation complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter-Friendly Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(\n",
    "    data_path: str = DATA_PATH,\n",
    "    text_col: str = TEXT_COL,\n",
    "    label_col: str = LABEL_COL,\n",
    "    model_path: str = MODEL_PATH,\n",
    "    vectorizer_path: str = VECTORIZER_PATH,\n",
    "    max_show: int = MAX_SHOW,\n",
    "):\n",
    "    \"\"\"\n",
    "    JUPYTER-FRIENDLY ENTRYPOINT.\n",
    "\n",
    "    Call this from a notebook to avoid argparse / ipykernel --f issues.\n",
    "\n",
    "    Example:\n",
    "        eval_df = run_evaluation()\n",
    "    \"\"\"\n",
    "    model, vectorizer = load_model_and_vectorizer(\n",
    "        model_path,\n",
    "        vectorizer_path,\n",
    "    )\n",
    "    if model is None or vectorizer is None:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = load_labeled_data(\n",
    "        csv_path=data_path,\n",
    "        text_col=text_col,\n",
    "        label_col=label_col,\n",
    "    )\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return evaluate_model_on_labeled(\n",
    "        df=df,\n",
    "        model=model,\n",
    "        vectorizer=vectorizer,\n",
    "        max_examples_to_show=max_show,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Execute this cell to evaluate the model. Results will print below, and `eval_df` will contain the evaluation DataFrame for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "eval_df = run_evaluation()\n",
    "\n",
    "# Display first few rows of results\n",
    "if not eval_df.empty:\n",
    "    print(\"\\nEvaluation DataFrame preview:\")\n",
    "    print(eval_df.head())\n",
    "else:\n",
    "    print(\"Evaluation failed. Check paths and files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}